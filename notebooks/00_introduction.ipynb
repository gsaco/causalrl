{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fd73ab",
   "metadata": {},
   "source": [
    "# 00 - Introduction: Estimand-First OPE\n",
    "\n",
    "This notebook is a fast, narrative tour of CausalRL. The key idea is\n",
    "estimand-first OPE: you declare the estimand and its assumptions, then every\n",
    "estimator and report is obligated to surface diagnostics that tell you whether\n",
    "the assumptions look plausible in the data.\n",
    "\n",
    "We run a small synthetic bandit experiment (so we know ground truth), inspect\n",
    "the report schema, and export a self-contained HTML report. Along the way we\n",
    "touch the public APIs for policies, data contracts, benchmarks, and\n",
    "experiment runners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a0c9e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Suggested environment:\n",
    "\n",
    "```\n",
    "pip install \"causalrl[plots]\"\n",
    "```\n",
    "\n",
    "(You can add `[docs]` or `[notebooks]` if you want full notebook tooling.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39327fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "import crl\n",
    "from crl.benchmarks.bandit_synth import SyntheticBandit, SyntheticBanditConfig\n",
    "from crl.benchmarks.harness import (\n",
    "    run_all_benchmarks,\n",
    "    run_bandit_benchmark,\n",
    "    run_mdp_benchmark,\n",
    ")\n",
    "from crl.benchmarks.mdp_synth import SyntheticMDP, SyntheticMDPConfig\n",
    "from crl.data import (\n",
    "    BanditDataset,\n",
    "    LoggedBanditDataset,\n",
    "    TrajectoryDataset,\n",
    "    TransitionDataset,\n",
    ")\n",
    "from crl.experiments import run_benchmark_suite, run_benchmarks_to_table\n",
    "from crl.ope import OpeReport, evaluate\n",
    "from crl.policies import (\n",
    "    BehaviorPolicy,\n",
    "    MLPConfig,\n",
    "    Policy,\n",
    "    StochasticPolicy,\n",
    "    TabularPolicy,\n",
    "    TorchMLPPolicy,\n",
    "    UniformPolicy,\n",
    ")\n",
    "from crl.utils.seeding import set_seed\n",
    "from crl.utils.validation import (\n",
    "    require_finite,\n",
    "    require_in_unit_interval,\n",
    "    require_ndarray,\n",
    "    require_same_length,\n",
    "    require_shape,\n",
    ")\n",
    "from crl.viz import configure_notebook_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afa25ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crl 0.1.0\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "np.random.seed(0)\n",
    "configure_notebook_display()\n",
    "\n",
    "print(\"crl\", crl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39220336",
   "metadata": {},
   "source": [
    "## Synthetic data for the tour\n",
    "\n",
    "We use the built-in synthetic bandit benchmark as a common source of logged\n",
    "data for policy and estimator demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc1f981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>stderr</th>\n",
       "      <th>ci</th>\n",
       "      <th>diagnostics</th>\n",
       "      <th>assumptions_checked</th>\n",
       "      <th>assumptions_flagged</th>\n",
       "      <th>warnings</th>\n",
       "      <th>metadata</th>\n",
       "      <th>lower_bound</th>\n",
       "      <th>upper_bound</th>\n",
       "      <th>estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.393095</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>(-0.4689600359090516, -0.3172303957405684)</td>\n",
       "      <td>{'overlap': {'min_behavior_prob': 0.1097201314...</td>\n",
       "      <td>[sequential_ignorability, overlap]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'estimator': 'IS', 'num_samples': 1000, 'requ...</td>\n",
       "      <td>-0.468960</td>\n",
       "      <td>-0.317230</td>\n",
       "      <td>IS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.396854</td>\n",
       "      <td>0.042185</td>\n",
       "      <td>(-0.47953558159888593, -0.3141718825523601)</td>\n",
       "      <td>{'overlap': {'min_behavior_prob': 0.1097201314...</td>\n",
       "      <td>[sequential_ignorability, overlap]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'estimator': 'WIS', 'num_samples': 1000, 'req...</td>\n",
       "      <td>-0.479536</td>\n",
       "      <td>-0.314172</td>\n",
       "      <td>WIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.423104</td>\n",
       "      <td>0.030082</td>\n",
       "      <td>(-0.48206487086077743, -0.36414260107394003)</td>\n",
       "      <td>{'overlap': {'min_behavior_prob': 0.1097201314...</td>\n",
       "      <td>[sequential_ignorability, overlap]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'estimator': 'DoubleRL', 'config': {'num_fold...</td>\n",
       "      <td>-0.482065</td>\n",
       "      <td>-0.364143</td>\n",
       "      <td>DoubleRL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value    stderr                                            ci  \\\n",
       "0 -0.393095  0.038707    (-0.4689600359090516, -0.3172303957405684)   \n",
       "1 -0.396854  0.042185   (-0.47953558159888593, -0.3141718825523601)   \n",
       "2 -0.423104  0.030082  (-0.48206487086077743, -0.36414260107394003)   \n",
       "\n",
       "                                         diagnostics  \\\n",
       "0  {'overlap': {'min_behavior_prob': 0.1097201314...   \n",
       "1  {'overlap': {'min_behavior_prob': 0.1097201314...   \n",
       "2  {'overlap': {'min_behavior_prob': 0.1097201314...   \n",
       "\n",
       "                  assumptions_checked assumptions_flagged warnings  \\\n",
       "0  [sequential_ignorability, overlap]                  []       []   \n",
       "1  [sequential_ignorability, overlap]                  []       []   \n",
       "2  [sequential_ignorability, overlap]                  []       []   \n",
       "\n",
       "                                            metadata  lower_bound  \\\n",
       "0  {'estimator': 'IS', 'num_samples': 1000, 'requ...    -0.468960   \n",
       "1  {'estimator': 'WIS', 'num_samples': 1000, 'req...    -0.479536   \n",
       "2  {'estimator': 'DoubleRL', 'config': {'num_fold...    -0.482065   \n",
       "\n",
       "   upper_bound estimator  \n",
       "0    -0.317230        IS  \n",
       "1    -0.314172       WIS  \n",
       "2    -0.364143  DoubleRL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark = SyntheticBandit(SyntheticBanditConfig(seed=0))\n",
    "dataset = benchmark.sample(num_samples=1_000, seed=1)\n",
    "true_value = benchmark.true_policy_value(benchmark.target_policy)\n",
    "\n",
    "pd.DataFrame([dataset.describe()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ddfca9",
   "metadata": {},
   "source": [
    "## Policy interfaces\n",
    "\n",
    "Policies expose action probabilities, log-probabilities, and sampling. Below\n",
    "we create uniform, stochastic, tabular, torch-backed, and behavior policy\n",
    "wrappers and display a snapshot of their action probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "sample_contexts = dataset.contexts[:6]\n",
    "\n",
    "uniform_policy = UniformPolicy(action_space_n=dataset.action_space_n)\n",
    "\n",
    "\n",
    "def softmax_probs(obs: np.ndarray) -> np.ndarray:\n",
    "    obs = np.asarray(obs)\n",
    "    obs_2d = obs if obs.ndim == 2 else obs.reshape(-1, 1)\n",
    "    logits = np.zeros((obs_2d.shape[0], dataset.action_space_n), dtype=float)\n",
    "    logits[:, 0] = obs_2d[:, 0]\n",
    "    if dataset.action_space_n > 1:\n",
    "        logits[:, 1] = -obs_2d[:, 0]\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "stochastic_policy = StochasticPolicy(\n",
    "    prob_fn=softmax_probs, action_space_n=dataset.action_space_n, name=\"softmax_demo\"\n",
    ")\n",
    "\n",
    "# Tabular policies require discrete state indices.\n",
    "# We use a tiny two-state example to illustrate the interface.\n",
    "tabular_policy = TabularPolicy(np.array([[0.7, 0.3], [0.2, 0.8]]))\n",
    "tabular_states = np.array([0, 1, 0, 1, 1])\n",
    "\n",
    "mlp_policy = TorchMLPPolicy.from_config(\n",
    "    MLPConfig(\n",
    "        input_dim=sample_contexts.shape[1] if sample_contexts.ndim > 1 else 1,\n",
    "        action_dim=dataset.action_space_n,\n",
    "        hidden_sizes=(16, 16),\n",
    "        activation=\"tanh\",\n",
    "    )\n",
    ")\n",
    "\n",
    "behavior_policy = BehaviorPolicy(\n",
    "    policy=uniform_policy, source=\"known\", diagnostics={\"note\": \"demo\"}\n",
    ")\n",
    "\n",
    "sample_actions = uniform_policy.sample_action(sample_contexts[:4], rng)\n",
    "log_probs = uniform_policy.log_prob(sample_contexts[:4], sample_actions)\n",
    "\n",
    "policy_rows = [\n",
    "    {\n",
    "        \"policy\": \"uniform\",\n",
    "        \"sample_probs\": uniform_policy.action_probs(sample_contexts[:1])[0],\n",
    "    },\n",
    "    {\n",
    "        \"policy\": \"stochastic\",\n",
    "        \"sample_probs\": stochastic_policy.action_probs(sample_contexts[:1])[0],\n",
    "    },\n",
    "    {\n",
    "        \"policy\": \"tabular\",\n",
    "        \"sample_probs\": tabular_policy.action_probs(tabular_states[:1])[0],\n",
    "    },\n",
    "    {\n",
    "        \"policy\": \"torch_mlp\",\n",
    "        \"sample_probs\": mlp_policy.action_probs(sample_contexts[:1])[0],\n",
    "    },\n",
    "]\n",
    "\n",
    "pd.DataFrame(policy_rows), behavior_policy.to_dict(), log_probs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(uniform_policy, Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40990f",
   "metadata": {},
   "source": [
    "## Data contracts and validation helpers\n",
    "\n",
    "CausalRL ships data classes for logged bandits, trajectories, and transitions.\n",
    "We also provide validation helpers for shapes and probability bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe989ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_contexts = np.random.normal(size=(6, 2))\n",
    "manual_actions = np.array([0, 1, 0, 1, 0, 1])\n",
    "manual_rewards = np.random.normal(size=6)\n",
    "manual_behavior_probs = np.full(6, 0.5)\n",
    "\n",
    "require_ndarray(\"contexts\", manual_contexts)\n",
    "require_shape(\"actions\", manual_actions, 1)\n",
    "require_same_length(\n",
    "    [\"contexts\", \"actions\", \"rewards\"],\n",
    "    [manual_contexts, manual_actions, manual_rewards],\n",
    ")\n",
    "require_finite(\"rewards\", manual_rewards)\n",
    "require_in_unit_interval(\"behavior_action_probs\", manual_behavior_probs)\n",
    "\n",
    "manual_dataset = LoggedBanditDataset(\n",
    "    contexts=manual_contexts,\n",
    "    actions=manual_actions,\n",
    "    rewards=manual_rewards,\n",
    "    behavior_action_probs=manual_behavior_probs,\n",
    "    action_space_n=2,\n",
    "    metadata={\"source\": \"manual_demo\"},\n",
    ")\n",
    "\n",
    "roundtrip = LoggedBanditDataset.from_dict(manual_dataset.to_dict())\n",
    "\n",
    "pd.DataFrame([roundtrip.describe()]), isinstance(roundtrip, BanditDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_benchmark = SyntheticMDP(SyntheticMDPConfig(seed=5, horizon=4))\n",
    "traj_dataset = mdp_benchmark.sample(num_trajectories=5, seed=6)\n",
    "\n",
    "mask_flat = traj_dataset.mask.reshape(-1)\n",
    "obs_flat = traj_dataset.observations.reshape(\n",
    "    -1, *traj_dataset.observations.shape[2:]\n",
    ")[mask_flat]\n",
    "next_obs_flat = traj_dataset.next_observations.reshape(\n",
    "    -1, *traj_dataset.next_observations.shape[2:]\n",
    ")[mask_flat]\n",
    "actions_flat = traj_dataset.actions.reshape(-1)[mask_flat]\n",
    "rewards_flat = traj_dataset.rewards.reshape(-1)[mask_flat]\n",
    "dones_flat = traj_dataset.dones.reshape(-1)[mask_flat]\n",
    "behavior_probs_flat = (\n",
    "    traj_dataset.behavior_action_probs.reshape(-1)[mask_flat]\n",
    "    if traj_dataset.behavior_action_probs is not None\n",
    "    else None\n",
    ")\n",
    "\n",
    "episode_ids = np.repeat(\n",
    "    np.arange(traj_dataset.num_trajectories), traj_dataset.horizon\n",
    ")[mask_flat]\n",
    "timesteps = np.tile(np.arange(traj_dataset.horizon), traj_dataset.num_trajectories)[\n",
    "    mask_flat\n",
    "]\n",
    "\n",
    "transition_dataset = TransitionDataset(\n",
    "    states=obs_flat,\n",
    "    actions=actions_flat,\n",
    "    rewards=rewards_flat,\n",
    "    next_states=next_obs_flat,\n",
    "    dones=dones_flat,\n",
    "    behavior_action_probs=behavior_probs_flat,\n",
    "    discount=traj_dataset.discount,\n",
    "    action_space_n=traj_dataset.action_space_n,\n",
    "    episode_ids=episode_ids,\n",
    "    timesteps=timesteps,\n",
    "    metadata={\"source\": \"flattened_from_trajectory\"},\n",
    ")\n",
    "\n",
    "trajectory_roundtrip = transition_dataset.to_trajectory()\n",
    "\n",
    "(\n",
    "    pd.DataFrame([transition_dataset.describe()]),\n",
    "    pd.DataFrame([trajectory_roundtrip.describe()]),\n",
    "    isinstance(trajectory_roundtrip, TrajectoryDataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081265a4",
   "metadata": {},
   "source": [
    "## Quick OPE run\n",
    "\n",
    "We evaluate a target policy and compare estimators in a single call. The\n",
    "resulting report is a structured object that can serialize to a DataFrame or\n",
    "HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluate(dataset=dataset, policy=benchmark.target_policy)\n",
    "report.summary_table(), isinstance(report, OpeReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14468ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crl.core import EstimationReport\n",
    "\n",
    "first_report = next(iter(report.reports.values()))\n",
    "isinstance(first_report, EstimationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90544b7",
   "metadata": {},
   "source": [
    "## Visualization helpers\n",
    "\n",
    "OpeReport includes convenience plotting methods for estimator comparisons and\n",
    "weight diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_comparison = report.plot_estimator_comparison(truth=true_value)\n",
    "fig_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba934ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (\n",
    "    benchmark.target_policy.action_prob(dataset.contexts, dataset.actions)\n",
    "    / dataset.behavior_action_probs\n",
    ")\n",
    "fig_weights = report.plot_importance_weights(weights, logy=True)\n",
    "fig_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6dfb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_ess = report.plot_effective_sample_size(weights, by_time=False)\n",
    "fig_ess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e9743",
   "metadata": {},
   "source": [
    "## Report schema and HTML export\n",
    "\n",
    "Every estimator returns the same schema: point estimate, uncertainty\n",
    "(stderr/CI), diagnostics, and assumption flags. This makes it easy to compare\n",
    "methods side-by-side and to automate downstream checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89def2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('docs/assets/reports/intro_bandit_report.html')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = Path(\"docs/assets/reports\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "report_path = output_dir / \"intro_bandit_report.html\"\n",
    "report.save_html(str(report_path))\n",
    "report_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27dd54",
   "metadata": {},
   "source": [
    "## Benchmarks and experiment runners\n",
    "\n",
    "The benchmark harness runs synthetic bandit and MDP suites with known ground\n",
    "truth. Experiment helpers write CSV/JSONL outputs and aggregate tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_results = run_bandit_benchmark(num_samples=300, seed=0)\n",
    "mdp_results = run_mdp_benchmark(num_trajectories=80, seed=0)\n",
    "all_results = run_all_benchmarks(num_samples=300, num_trajectories=80, seed=0)\n",
    "\n",
    "pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_out = Path(\"docs/assets/benchmarks/intro\")\n",
    "bench_out.mkdir(parents=True, exist_ok=True)\n",
    "bench_records = run_benchmarks_to_table(\n",
    "    output_dir=bench_out, num_samples=200, num_trajectories=60, seed=0\n",
    ")\n",
    "\n",
    "pd.DataFrame(bench_records).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ce88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny custom suite to exercise run_benchmark_suite without long runtimes.\n",
    "custom_suite_dir = bench_out / \"suite_configs\"\n",
    "custom_suite_dir.mkdir(parents=True, exist_ok=True)\n",
    "custom_suite = {\n",
    "    \"suite\": \"intro_demo\",\n",
    "    \"benchmarks\": [\n",
    "        {\n",
    "            \"name\": \"bandit_tiny\",\n",
    "            \"type\": \"bandit\",\n",
    "            \"num_samples\": 200,\n",
    "            \"estimators\": [\"is\", \"wis\"],\n",
    "            \"behavior_known\": True,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "(custom_suite_dir / \"intro_demo.yaml\").write_text(\n",
    "    yaml.safe_dump(custom_suite), encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "suite_df = run_benchmark_suite(\n",
    "    suite=\"intro_demo\",\n",
    "    output_dir=bench_out / \"suite_run\",\n",
    "    seeds=[0],\n",
    "    config_dir=custom_suite_dir,\n",
    ")\n",
    "\n",
    "suite_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c656208",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- Policies, data contracts, and estimands form the backbone of OPE workflows.\n",
    "- Reports standardize metrics, diagnostics, and figures across estimators.\n",
    "- Benchmarks and experiment runners provide reproducible comparisons.\n",
    "- HTML export creates shareable artifacts for reviews."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
