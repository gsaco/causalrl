{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06b945c",
   "metadata": {},
   "source": [
    "# Bandit OPE End-to-End\n",
    "\n",
    "This notebook demonstrates a research-grade workflow:\n",
    "\n",
    "1. Define an explicit estimand and assumptions.\n",
    "2. Run multiple estimators with diagnostics.\n",
    "3. Compare against ground truth from a synthetic benchmark.\n",
    "4. Quantify sensitivity to unobserved confounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from crl.assumptions import AssumptionSet\n",
    "from crl.assumptions_catalog import BOUNDED_CONFOUNDING, OVERLAP, SEQUENTIAL_IGNORABILITY\n",
    "from crl.benchmarks.bandit_synth import SyntheticBandit, SyntheticBanditConfig\n",
    "from crl.estimands.policy_value import PolicyValueEstimand\n",
    "from crl.estimands.sensitivity_policy_value import SensitivityPolicyValueEstimand\n",
    "from crl.ope import evaluate\n",
    "from crl.viz import configure_notebook_display, save_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5a8c3",
   "metadata": {},
   "source": [
    "## 1) Generate a synthetic logged bandit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4944c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = SyntheticBandit(SyntheticBanditConfig(seed=0))\n",
    "dataset = bench.sample(num_samples=500, seed=1)\n",
    "dataset.describe()\n",
    "configure_notebook_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148a83b",
   "metadata": {},
   "source": [
    "## 2) Define the estimand (with assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimand = PolicyValueEstimand(\n",
    "    policy=bench.target_policy,\n",
    "    discount=1.0,\n",
    "    horizon=1,\n",
    "    assumptions=AssumptionSet([SEQUENTIAL_IGNORABILITY, OVERLAP]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532a66f",
   "metadata": {},
   "source": [
    "## 3) Run estimators + diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3709ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sensitivity = SensitivityPolicyValueEstimand(\n",
    "    policy=bench.target_policy,\n",
    "    discount=1.0,\n",
    "    horizon=1,\n",
    "    gammas=np.linspace(1.0, 2.0, 6),\n",
    "    assumptions=AssumptionSet([BOUNDED_CONFOUNDING]),\n",
    ")\n",
    "\n",
    "report = evaluate(\n",
    "    dataset=dataset,\n",
    "    policy=bench.target_policy,\n",
    "    estimand=estimand,\n",
    "    estimators=[\"is\", \"wis\", \"double_rl\"],\n",
    "    sensitivity=sensitivity,\n",
    ")\n",
    "\n",
    "summary = report.to_dataframe()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    summary[[\"estimator\", \"value\", \"lower_bound\", \"upper_bound\"]]\n",
    "    .round(3)\n",
    "    .to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d54dce",
   "metadata": {},
   "source": [
    "## 4) Compare to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c26d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_value = bench.true_policy_value(bench.target_policy)\n",
    "print(f\"True policy value: {true_value:.3f}\")\n",
    "fig_comp = report.plot_estimator_comparison(truth=true_value)\n",
    "fig_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586df1aa",
   "metadata": {},
   "source": [
    "## 5) Weight diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_probs = bench.target_policy.action_prob(dataset.contexts, dataset.actions)\n",
    "weights = target_probs / dataset.behavior_action_probs\n",
    "fig_weights = report.plot_importance_weights(weights)\n",
    "fig_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb10698",
   "metadata": {},
   "source": [
    "## 6) Sensitivity bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867955ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report.figures.get(\"sensitivity_bounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24260147",
   "metadata": {},
   "source": [
    "## 7) Export figures for the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure(fig_comp, \"docs/assets/figures/bandit_end_to_end_estimator_comparison\")\n",
    "save_figure(fig_weights, \"docs/assets/figures/bandit_end_to_end_weights\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
